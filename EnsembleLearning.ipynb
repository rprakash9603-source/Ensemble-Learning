{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "1nbYWNwKnEuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# 2. Train a Random Forest Classifier\n",
        "# We'll use 100 trees and a random_state for reproducibility\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X, y)\n",
        "\n",
        "# 3. Get feature importance scores\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to view feature names and their scores together\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance and get the top 5\n",
        "top_5_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "# Print the results\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(\"-\" * 30)\n",
        "print(top_5_features.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOHKYwXMnMJc",
        "outputId": "d16e82a7-4df5-4636-8b82-26c63e4a2967"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "------------------------------\n",
            "             Feature  Importance\n",
            "          worst area    0.139357\n",
            "worst concave points    0.132225\n",
            " mean concave points    0.107046\n",
            "        worst radius    0.082848\n",
            "     worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "h1_3naG8nnr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train a Single Decision Tree\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "tree_preds = single_tree.predict(X_test)\n",
        "tree_acc = accuracy_score(y_test, tree_preds)\n",
        "\n",
        "# 3. Train a Bagging Classifier using Decision Trees\n",
        "# n_estimators=50 means we are ensemble-ing 50 different trees\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
        "                                  n_estimators=50,\n",
        "                                  random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_preds = bagging_model.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "# 4. Compare Results\n",
        "print(f\"Accuracy of a Single Decision Tree: {tree_acc:.4f}\")\n",
        "print(f\"Accuracy of Bagging Classifier (50 trees): {bagging_acc:.4f}\")\n",
        "\n",
        "if bagging_acc > tree_acc:\n",
        "    print(\"\\nResult: The Bagging ensemble outperformed the single tree.\")\n",
        "elif bagging_acc == tree_acc:\n",
        "    print(\"\\nResult: Both models achieved the same accuracy on this split.\")\n",
        "else:\n",
        "    print(\"\\nResult: The single tree performed better (this can happen on very small/simple datasets).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eql9m6sMnvbO",
        "outputId": "3a399d6b-b51e-489a-b944-6e5af59afe20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of a Single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier (50 trees): 1.0000\n",
            "\n",
            "Result: Both models achieved the same accuracy on this split.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "5wyM1T30n550"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Define the model and the parameter grid\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30]   # Maximum depth of the trees\n",
        "}\n",
        "\n",
        "# 3. Initialize GridSearchCV\n",
        "# cv=5 means 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# 4. Train/Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Extract Best Parameters and Evaluate\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict using the best model found by the grid search\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters Found:\")\n",
        "print(f\"- max_depth: {best_params['max_depth']}\")\n",
        "print(f\"- n_estimators: {best_params['n_estimators']}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Final Accuracy on Test Set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrctMYE4n-v8",
        "outputId": "ce229d0f-1102-4f8a-e739-45b4d8571943"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters Found:\n",
            "- max_depth: None\n",
            "- n_estimators: 200\n",
            "------------------------------\n",
            "Final Accuracy on Test Set: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "Mw4aFlzqohK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train a Bagging Regressor\n",
        "# We use 100 Decision Trees as our base estimators\n",
        "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(),\n",
        "                               n_estimators=100,\n",
        "                               random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bag_pred = bagging_reg.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "\n",
        "# 3. Train a Random Forest Regressor\n",
        "# Random Forest also uses 100 trees but adds feature randomness\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# 4. Compare Results\n",
        "print(f\"Mean Squared Error (Bagging Regressor): {bag_mse:.4f}\")\n",
        "print(f\"Mean Squared Error (Random Forest Regressor): {rf_mse:.4f}\")\n",
        "\n",
        "# Calculate percentage improvement\n",
        "improvement = ((bag_mse - rf_mse) / bag_mse) * 100\n",
        "print(f\"\\nRandom Forest reduced error by {improvement:.2f}% compared to standard Bagging.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KGyh41ooose",
        "outputId": "57418b3f-9f39-4018-fdcb-04fbefb0d1dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.2559\n",
            "Mean Squared Error (Random Forest Regressor): 0.2554\n",
            "\n",
            "Random Forest reduced error by 0.22% compared to standard Bagging.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n"
      ],
      "metadata": {
        "id": "pCcYabAlo9sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: In the financial sector, predicting loan defaults is a high-stakes task where both accuracy (identifying potential defaulters) and interpretability (knowing why a loan was denied) are crucial.\n",
        "\n",
        "Here is the step-by-step strategic approach:\n",
        "\n",
        "1. Choosing Between Bagging and Boosting\n",
        "For loan default prediction, I would choose Boosting (specifically XGBoost or LightGBM).\n",
        "\n",
        "Reasoning: Loan datasets are often imbalanced (most people don't default). Boosting is superior here because it focuses on \"hard-to-classify\" cases—the subtle patterns that distinguish a risky borrower from a safe one—by sequentially correcting errors from previous trees.\n",
        "\n",
        "2. Handling Overfitting\n",
        "Ensemble models can overfit if allowed to grow too complex. I would manage this by:\n",
        "\n",
        "Tree Constraints: Limiting the max_depth to prevent trees from memorizing specific transactions.\n",
        "\n",
        "Learning Rate (Shrinkage): Using a small learning rate (e.g., 0.01) so the model learns slowly and generalizes better.\n",
        "\n",
        "Early Stopping: Monitoring validation error and stopping training when the error stops decreasing.\n",
        "\n",
        "3. Selecting Base Models\n",
        "I would use Decision Trees as base models.\n",
        "\n",
        "Reasoning: They handle non-linear relationships well (e.g., a borrower might be \"low risk\" if they have a medium income AND low debt, but \"high risk\" if they have high income AND high debt).\n",
        "\n",
        "4. Evaluating Performance via Cross-Validation\n",
        "I would use Stratified K-Fold Cross-Validation.\n",
        "\n",
        "Since defaults are rare, \"stratified\" ensures each fold has the same percentage of defaulters as the original data, providing a more reliable estimate of how the model will perform on new customers.\n",
        "\n",
        "5. Justifying Ensemble Learning in Finance\n",
        "Ensemble learning reduces the \"model risk\" associated with any single algorithm. In a bank, a single decision tree might deny a loan based on one arbitrary cutoff (e.g., Credit Score < 650). An ensemble aggregates hundreds of perspectives, ensuring that the final decision is based on a consensus of evidence, leading to fairer and more accurate risk assessment."
      ],
      "metadata": {
        "id": "RQbPmHF_pUDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Simulate a loan default dataset\n",
        "# (1000 customers, 20 features like income, debt-to-income ratio, etc.)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, weights=[0.8, 0.2], random_state=42)\n",
        "\n",
        "# 2. Define the Boosting Model (XGBoost logic via GradientBoostingClassifier)\n",
        "# We limit max_depth and use a learning_rate to handle overfitting\n",
        "boosting_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 3. Evaluate using Stratified Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(boosting_model, X, y, cv=skf, scoring='f1')\n",
        "\n",
        "# 4. Fit the model to see feature importances (Simulated)\n",
        "boosting_model.fit(X, y)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Cross-Validation F1-Scores: {cv_scores}\")\n",
        "print(f\"Mean F1-Score: {np.mean(cv_scores):.4f}\")\n",
        "print(\"\\nTop 5 Feature Importance Scores:\")\n",
        "for i in range(5):\n",
        "    print(f\"Feature {i}: {boosting_model.feature_importances_[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJHHbS2LpjIV",
        "outputId": "d4cfaa0d-8cee-45a6-8640-ed75164ea36e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation F1-Scores: [0.6875     0.64516129 0.61290323 0.71232877 0.6875    ]\n",
            "Mean F1-Score: 0.6691\n",
            "\n",
            "Top 5 Feature Importance Scores:\n",
            "Feature 0: 0.0343\n",
            "Feature 1: 0.0629\n",
            "Feature 2: 0.0751\n",
            "Feature 3: 0.0221\n",
            "Feature 4: 0.0719\n"
          ]
        }
      ]
    }
  ]
}